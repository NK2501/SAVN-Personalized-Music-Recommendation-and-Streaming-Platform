{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a4dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Configuration (Customize these values)\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "KAFKA_TOPIC_NAME = 'music-data'\n",
    "MONGO_HOST = 'localhost'\n",
    "MONGO_PORT = 27017\n",
    "MONGO_DB_NAME = 'project'\n",
    "MONGO_COLLECTION_NAME = 'music_dataset'\n",
    "\n",
    "# Define the schema for music data (Customize based on your actual data)\n",
    "schema = {\n",
    "    \"title\": \"string\",\n",
    "    \"artist\": \"string\",\n",
    "    \"genre\": \"string\",\n",
    "    \"duration_seconds\": \"integer\",\n",
    "    \"play_count\": \"long\",\n",
    "    \"release_date\": \"string\",\n",
    "    \"album\": \"string\",\n",
    "    \"year\": \"string\",\n",
    "    \"label\": \"string\",\n",
    "    \"rating\": \"string\"\n",
    "}\n",
    "\n",
    "# Create a SparkSession with proper error handling\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"KafkaToMongoDB\") \\\n",
    "        .getOrCreate()\n",
    "except Exception as e:\n",
    "    print(f\"Error creating SparkSession: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Read from Kafka topic using the kafka format\n",
    "try:\n",
    "    df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "        .option(\"subscribe\", KAFKA_TOPIC_NAME) \\\n",
    "        .option(\"startingOffsets\", \"latest\") \\\n",
    "        .load()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from Kafka topic: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Parse JSON and select relevant columns with data type validation\n",
    "try:\n",
    "    df = df.select(from_json(col(\"value\").cast(\"string\"), \"schema\").alias(\"data\")).select(\"data.*\")\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing JSON or selecting columns: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Connect to MongoDB with error handling and authentication (if needed)\n",
    "try:\n",
    "    client = MongoClient(MONGO_HOST, MONGO_PORT)  # Authentication can be added here\n",
    "    db = client[MONGO_DB_NAME]\n",
    "    collection = db[MONGO_COLLECTION_NAME]\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to MongoDB: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Write data to MongoDB with batching, checkpointing, and error handling\n",
    "try:\n",
    "    writeStream = df \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .foreachBatch(lambda batchDF, batchId: batchDF.foreach(lambda row: collection.insert_one(row.asDict()))) \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/spark-checkpoint\") \\\n",
    "        .start()\n",
    "    writeStream.awaitTermination()\n",
    "except Exception as e:\n",
    "    print(f\"Error writing data to MongoDB: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Stop SparkSession gracefully\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
